{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46997dc0",
   "metadata": {},
   "source": [
    "##  Web scraping en Python\n",
    "\n",
    "Le **web scraping** consiste à extraire automatiquement des données depuis des pages web.\n",
    "\n",
    "### Étapes principales\n",
    "\n",
    "1. **Envoyer une requête HTTP**  \n",
    "   Utiliser une bibliothèque comme `requests` pour télécharger le contenu d’une page web.\n",
    "\n",
    "2. **Analyser le contenu HTML**  \n",
    "   Utiliser `BeautifulSoup` pour parser et naviguer dans le code HTML.\n",
    "\n",
    "3. **Extraire les données**  \n",
    "   Repérer les balises ou attributs qui contiennent les informations recherchées et les extraire.\n",
    "\n",
    "4. **Stocker les données**  \n",
    "   Sauvegarder les données extraites dans un fichier (CSV, Excel, base de données, etc.).\n",
    "\n",
    "### Bibliothèques courantes\n",
    "\n",
    "- `requests` : pour télécharger les pages web.\n",
    "- `BeautifulSoup` : pour analyser et extraire les données du HTML.\n",
    "- `pandas` : pour organiser et stocker les données.\n",
    "\n",
    "### Exemple simple\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://example.com\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Extraire tous les titres h1\n",
    "titres = [h1.text for h1 in soup.find_all(\"h1\")]\n",
    "print(titres)\n",
    "```\n",
    "\n",
    "### Bonnes pratiques\n",
    "\n",
    "- Respecter les conditions d’utilisation du site (robots.txt).\n",
    "- Ne pas surcharger les serveurs (utiliser des délais entre les requêtes).\n",
    "- Vérifier la légalité du scraping selon le site et le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2246dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Search code, repositories, users, issues, pull requests...', '\\n        Provide feedback\\n      ', '\\n        Saved searches\\n      ', 'Build and ship software on a single, collaborative platform']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://github.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Extraire tous les titres h1\n",
    "titres = [h1.text for h1 in soup.find_all(\"h1\")]\n",
    "print(titres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4df0a7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statut de la réponse : 200\n"
     ]
    }
   ],
   "source": [
    "# URL de la page à scraper\n",
    "url = \"https://fr.wikipedia.org/wiki/Liste_des_langages_de_programmation\"\n",
    "\n",
    "# En-têtes pour simuler un navigateur\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Faire la requête GET\n",
    "try:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()  # Lève une exception pour les codes 4XX/5XX\n",
    "    print(f\"Statut de la réponse : {response.status_code}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Erreur lors de la requête : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43dadba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titre de la page : Liste de langages de programmation — Wikipédia\n"
     ]
    }
   ],
   "source": [
    "# Créer un objet BeautifulSoup à partir du contenu HTML\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Afficher le titre de la page pour vérifier\n",
    "print(f\"Titre de la page : {soup.title.string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f7d99ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 premiers langages trouvés :\n"
     ]
    }
   ],
   "source": [
    "# Trouver tous les éléments <li> dans la page\n",
    "all_languages = []\n",
    "\n",
    "# La page Wikipedia a plusieurs listes, nous ciblons celles dans les sections principales\n",
    "sections = soup.find_all('div', class_='div-col')\n",
    "\n",
    "for section in sections:\n",
    "    languages = section.find_all('li')\n",
    "    for lang in languages:\n",
    "        # Nettoyer le texte et extraire le nom du langage\n",
    "        language_name = lang.get_text().strip()\n",
    "        # Certains éléments contiennent des notes entre parenthèses\n",
    "        if '(' in language_name:\n",
    "            language_name = language_name.split('(')[0].strip()\n",
    "        all_languages.append(language_name)\n",
    "\n",
    "# Afficher les 10 premiers langages\n",
    "print(\"10 premiers langages trouvés :\")\n",
    "for lang in all_languages[:10]:\n",
    "    print(f\"- {lang}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
